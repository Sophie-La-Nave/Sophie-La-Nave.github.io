layout: post
title: "Day 5 - Modules 5 and 6"
---

### What I Set Out to Do
I was aiming to complete the entirety of the fifth and sixth course module. The focus of this fifth section was to introduce the concepts of Polling, Confidence Intervals, and the Normal Distributionand, and key ideas within it. The purpose of the sixth module was to introduce Machine Learning (ML).

---

### What I Actually Did

Modules 5 and 6.
Summary of the 7 lessons in Module 5:

1. Random variables are variables generated by chance. Notation: capital letters, usually X. Discrete variables are whole numbers, continuous variables can have infinetely many values. An example of a random variable can be rolling a die, and the variable you're aiming for is random. 

2. Bernoulli and Binomial Distributions. Bernoulli distributions are doing something once with fixed outcomes. Eg, trying to roll a die once, and trying to roll a 6, y~Bernoulli(p = 1/6). Whereas binomial distributions are the same things but multiple times. Binomial (n, p) ; number of times, probability. Bernoulli = Binomial distribution when n=1.  

3. Python Functions for Random Distributions. As shown below, you import scipy (scientific python) library to find distributions. Normal, Binomial, and Bernoulli. You can use __.cdf, __.ppt, and __.rvs tp return: cumulative distribution frequemcy (probability of number occuting below input), probability point (input is probability, output is the point that that probability occurs), and random value sample (no input, random draw from distribution).
Normal Distribution	from scipy.stats import norm
N = norm()   # Normal Distribution

Bernoulli Distribution	
from scipy.stats import bernoulli
B = bernoulli(p=0.2)   # Bernoulli Distribution with p=0.2

Binomial Distribution	
from scipy.stats import binom
D = binom(p=0.1, n=50)   # Binomial Distribution with p=0.1, n=50

4. Central limit theorem; if we repeatedly take individual random sample size n from any distribution/population, distribution of sample means approaches normal curve. This is useful because we can use the normal curve as a reference point for data calculations/stats.

5. Polling and Sampling. The keys to polling are: 1, random sampling. 2, reducing bias. and 3, reduce chance error. Inference is used in sampling to get a greater idea of a population, even when the population is not fully available. Inference is using a random sample population to make estimates about the population sampled from.

6. Confidence samples are related to polling. Confidence intervals are sample % + or - z-score*SE. AKA Sample % + - margin of error. SE stands for standard error.

7. Hypothesis testing is deciding whether or not the hypothesis is true or if a difference in results from T (treatment) and C (control) can be random. Always assume null hypothesis (no effect/T didn't work) is true until proven otherwise.
Steps to hypothesis testing:
  a. State null (no effect/T didn't work) and alternate (T did work) hypothesese. AKA firgure out: What are we testing?
  b. Calculate test statistic; expected result vs. what we got.
  c. Find P-value (probability we would get this outcome by chance)
  d. Make conclusions.

Summary of the 6 lessons in Module 6:

1. Machine Learning (ML). AKA AI. ML functions by using data to construct an algorithm -> constructing a model -> conclusions.
All AI fits into this Table:

                          | Unsupervised ML     |   Supervised ML

__________________________|_____________________|_________________

                          | No 'corect' answer  | Known 'correct
                          | or expected outcome.| expected answer
                          |                     | provided in 
                          |                     | training data.

__________________________|_____________________|_________________

Prediction algorithm      |                     | Linear regression.
 - predicts an exact val. |                     |
    for algorithm         |                     |

__________________________|_____________________|__________________

Classification            | k-means clustering  |
- places into category or |                     |
    class                 |                     |

2. Correlation. Correlation coefficient (r), measures stregnth of linear relationships. -1 <= r <= 1. r = 0 : no correlation, r > 0 : pos correlation, r < 0 : negative correlation. 

3. Linear regression. Using a variable to predict another variable. eg, x to predict y. Regression line/line of best fit makes errors the smallest by going down the middle in a linear plot. Predictions can be made based on the line of best fit (eg. x val = __, so y val = __). x+y = simple linear regression. Many x's + y = multiple regression.

4. Machine learning models in python w/ Scikit-learn (sk-learn). 
  a. create an instance of the model
    Code: model = LinearRegression(__)
  b. Train model on data
    Code: model.fit(df[["height"]], df["weight"])
                    independent var.   dependent var.
  c. Predict future data
    Code: model.predict(___)
                        independent var = input, returns dependent var

5. Clustering. Clustering uses k-means. k = 4, means 4 clusters, k = 2, means 2 clusters, and so on. Clusters are groupings of data points on a graph. Clusters are great at recognizing anomolies/outliers. In real life these programs for clusters are used to recognize fraud, because anamolous spending will appear in a different cluster than expected.

6. Towards ML in Python. Scikit learn is the main python library for ML. It's used for classification, regression, and clustering. It always uses the steps shown in Module 6 lesson 4. (ie, .fit, and .predict)

---

### What I Learned or Noticed
I learned that Machine learning is a complex subject to understand, but the applications in python are actually quite simple and significant. The most important thing is to understand the significance of your data, then from there you just have to run some code and come to conclussions.
### What's Next

My tip/piece of advice!!
